import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib

matplotlib.use('Agg')

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib


class RPropMLPEstimator(BaseEstimator, RegressorMixin):
    def __init__(self, n_hidden_layers=5, n_neurons=10, max_iter=100,
                 eta_plus=1.2, eta_minus=0.5, delta_min=1e-6, delta_max=50.0,
                 random_state=42, patience=20, min_delta=1e-4, dropout_rate=0.001):
        self.n_hidden_layers = n_hidden_layers
        self.n_neurons = n_neurons
        self.max_iter = max_iter
        self.eta_plus = eta_plus
        self.eta_minus = eta_minus
        self.delta_min = delta_min
        self.delta_max = delta_max
        self.random_state = random_state
        self.patience = patience
        self.min_delta = min_delta
        self.dropout_rate = dropout_rate
        self.history = {'train_loss': [], 'val_loss': []}
        np.random.seed(random_state)

    def _initialize_weights(self, input_dim):
        self.weights = []
        self.prev_gradients = []
        self.deltas = []

        w = np.random.randn(input_dim, self.n_neurons) * np.sqrt(2.0 / input_dim)
        self.weights.append(w)
        self.prev_gradients.append(np.zeros_like(w))
        self.deltas.append(np.ones_like(w) * 0.1)

        for i in range(self.n_hidden_layers - 1):
            w = np.random.randn(self.n_neurons, self.n_neurons) * np.sqrt(2.0 / self.n_neurons)
            self.weights.append(w)
            self.prev_gradients.append(np.zeros_like(w))
            self.deltas.append(np.ones_like(w) * 0.1)

        w = np.random.randn(self.n_neurons, 1) * np.sqrt(2.0 / self.n_neurons)
        self.weights.append(w)
        self.prev_gradients.append(np.zeros_like(w))
        self.deltas.append(np.ones_like(w) * 0.1)

    def _dropout(self, X, training=True):
        if training and self.dropout_rate > 0:
            mask = np.random.binomial(1, 1 - self.dropout_rate, size=X.shape) / (1 - self.dropout_rate)
            return X * mask
        return X

    def _forward_pass(self, X, training=True):
        self.activations = [X]
        self.z_values = []
        self.dropout_masks = []

        z = np.dot(X, self.weights[0])
        self.z_values.append(z)
        h = np.tanh(z)
        h = self._dropout(h, training)
        self.activations.append(h)

        for i in range(1, self.n_hidden_layers):
            z = np.dot(h, self.weights[i])
            self.z_values.append(z)
            h = np.tanh(z)
            h = self._dropout(h, training)
            self.activations.append(h)

        output = np.dot(h, self.weights[-1])
        self.z_values.append(output)
        self.activations.append(output)

        return output

    def _backward_pass(self, X, y, output):
        m = X.shape[0]
        gradients = []

        error = output - y.reshape(-1, 1)
        grad = np.dot(self.activations[-2].T, error) / m
        gradients.insert(0, grad)

        for i in range(len(self.weights) - 2, -1, -1):
            error = np.dot(error, self.weights[i + 1].T) * (1 - np.tanh(self.z_values[i]) ** 2)
            grad = np.dot(self.activations[i].T, error) / m
            gradients.insert(0, grad)

        return gradients

    def _calculate_loss(self, y_true, y_pred):
        return np.mean(np.square(y_true.reshape(-1, 1) - y_pred))

    def _update_weights(self, gradients):
        for i in range(len(self.weights)):
            sign = np.sign(gradients[i] * self.prev_gradients[i])

            self.deltas[i] = np.where(sign > 0,
                                      np.minimum(self.deltas[i] * self.eta_plus, self.delta_max),
                                      np.where(sign < 0,
                                               np.maximum(self.deltas[i] * self.eta_minus, self.delta_min),
                                               self.deltas[i]))

            update = -np.sign(gradients[i]) * self.deltas[i]
            self.weights[i] += update

            self.prev_gradients[i] = np.where(sign < 0, 0, gradients[i])

    def fit(self, X, y):
        try:
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=0.2, random_state=self.random_state
            )

            self._initialize_weights(X.shape[1])

            best_loss = float('inf')
            patience_counter = 0
            best_weights = None

            for epoch in range(self.max_iter):
                train_output = self._forward_pass(X_train, training=True)
                gradients = self._backward_pass(X_train, y_train, train_output)
                self._update_weights(gradients)

                train_loss = self._calculate_loss(y_train, train_output)
                val_output = self._forward_pass(X_val, training=False)
                val_loss = self._calculate_loss(y_val, val_output)

                self.history['train_loss'].append(train_loss)
                self.history['val_loss'].append(val_loss)

                if val_loss < best_loss - self.min_delta:
                    best_loss = val_loss
                    patience_counter = 0
                    best_weights = [w.copy() for w in self.weights]
                else:
                    patience_counter += 1

                if patience_counter >= self.patience:
                    if best_weights is not None:
                        self.weights = best_weights
                    break

            return self

        except Exception as e:
            print(f"Error during fitting: {str(e)}")
            raise

    def predict(self, X):
        try:
            return self._forward_pass(X, training=False)
        except Exception as e:
            print(f"Error during prediction: {str(e)}")
            raise

def create_publication_quality_plots(best_model, target_column, metrics, early_stopping_epoch):
    plt.style.use('default')
    plt.rcParams.update({
        'figure.facecolor': 'white',
        'axes.facecolor': 'white',
        'axes.edgecolor': 'black',
        'axes.labelcolor': 'black',
        'axes.grid': True,
        'grid.color': 'gray',
        'grid.linestyle': '--',
        'grid.alpha': 0.3,
        'lines.linewidth': 2.5,
        'font.family': 'sans-serif',
        'font.size': 14,  # Genel font boyutu artırıldı (12 -> 14)
        'text.color': 'black',
        'xtick.color': 'black',
        'ytick.color': 'black',
        'xtick.direction': 'out',
        'ytick.direction': 'out',
        'axes.spines.top': True,
        'axes.spines.right': True
    })

    safe_target = target_column.replace(" ", "_").replace("(", "").replace(")", "")

    fig, ax = plt.subplots(figsize=(12, 10), dpi=300)

    ax.plot(best_model.history['train_loss'],
            label='DMLP Training MSE',
            color='#2E86C1',  
            linewidth=3.5,  
            alpha=0.9,  
            zorder=3)

    ax.plot(best_model.history['val_loss'],
            label='DMLP Validation MSE',
            color='#E74C3C',  
            linewidth=3.5,  
            alpha=0.9,  
            zorder=3)

    ax.axvline(x=early_stopping_epoch,
               color='#009988',
               linestyle='--',
               linewidth=2.5,  
               alpha=0.8,  
               label=f'Early Stopping Point\n(Epoch {early_stopping_epoch})',
               zorder=2)

    ax.scatter(early_stopping_epoch,
               best_model.history['val_loss'][early_stopping_epoch],
               color='#009988',
               s=200,  
               zorder=4,
               edgecolor='white',
               linewidth=2.5,  
               marker='o')  

    ax.set_xlabel('Training Epochs', fontsize=17, fontweight='bold')
    ax.set_ylabel('Mean Squared Error (MSE)', fontsize=17, fontweight='bold')

    title = f'DMLP Training Convergence Analysis\n{target_column}'
    subtitle = (f'$R^2_{{\mathrm{{train}}}}$ = {metrics["train_r2"]:.4f}, '
                f'$R^2_{{\mathrm{{test}}}}$ = {metrics["test_r2"]:.4f}\n'
                f'$\mathrm{{RMSE}}_{{train}}$ = {metrics["train_rmse"]:.2e}, '
                f'$\mathrm{{RMSE}}_{{test}}$ = {metrics["test_rmse"]:.2e}')

    ax.set_title(title + '\n' + subtitle,
                 fontsize=19,
                 fontweight='bold',
                 pad=20)

    ax.tick_params(axis='both',
                   labelsize=14,
                   width=2,  
                   length=6,  
                   labelcolor='black',
                   colors='black')

    ax.grid(True,
            linestyle='--',
            alpha=0.4,  
            linewidth=1.2,  
            color='gray',
            zorder=1)

    ax.legend(loc='upper right',
              fontsize=14,
              frameon=True,
              fancybox=True,
              framealpha=0.95,
              edgecolor='gray',
              shadow=True,
              borderpad=1,  
              labelspacing=1.2,  
              handlelength=3,  
              handletextpad=0.8)  

    for spine in ax.spines.values():
        spine.set_linewidth(1.5)

    plt.tight_layout()
    plt.savefig(f'DMLP_training_history_{safe_target}.png',
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none')
    plt.close()

    def create_publication_quality_plots(best_model, target_column, metrics, early_stopping_epoch):
        fig, ax = plt.subplots(figsize=(12, 10), dpi=300)

        ax.scatter(metrics['y_train_true'],
                   metrics['y_train_pred'],
                   c='green',
                   alpha=0.7,
                   s=120, 
                   marker='o',
                   label='Train Data',
                   zorder=3)

        ax.scatter(metrics['y_test_true'],
                   metrics['y_test_pred'],
                   c='blue',
                   alpha=0.7,
                   s=120, 
                   marker='x',
                   label='Test Data',
                   zorder=3)

        max_val = max(np.max(metrics['y_train_true']), np.max(metrics['y_train_pred']),
                      np.max(metrics['y_test_true']), np.max(metrics['y_test_pred']))
        min_val = min(np.min(metrics['y_train_true']), np.min(metrics['y_train_pred']),
                      np.min(metrics['y_test_true']), np.min(metrics['y_test_pred']))
        padding = (max_val - min_val) * 0.1

        ax.plot([min_val - padding, max_val + padding],
                [min_val - padding, max_val + padding],
                'k--',
                linewidth=2.0,
                label='Perfect Prediction',
                zorder=2)

        ax.set_xlabel('Experimental Values', fontsize=17, fontweight='bold')
        ax.set_ylabel('Predicted Values', fontsize=17, fontweight='bold')

        ax.set_title('DMLP - Experimental vs Predicted Values',
                     fontsize=19,
                     fontweight='bold',
                     pad=20)

        ax.tick_params(axis='both', labelsize=14)

        metrics_text = (
            f'R² = {metrics["test_r2"]:.4f}\n'
            f'RMSE = {metrics["test_rmse"]:.4f}\n'
            f'MAE = {metrics["test_mae"]:.4f}\n'
            f'MAPE = {metrics["test_mape"]:.2f}%'
        )
        ax.text(0.95, 0.05, metrics_text,
                transform=ax.transAxes,
                verticalalignment='bottom',
                horizontalalignment='right',
                fontsize=17,
                fontweight='bold',
                fontname='Arial',
                bbox=dict(boxstyle='round',
                          facecolor='white',
                          alpha=0.8,
                          pad=1.0))  

        ax.grid(True, linestyle='--', alpha=0.7, zorder=1)
        ax.legend(fontsize=14,
                  loc='upper left',
                  frameon=True,  
                  framealpha=0.9,
                  edgecolor='gray',
                  fancybox=True,
                  shadow=True)

        ax.set_xlim([min_val - padding, max_val + padding])
        ax.set_ylim([min_val - padding, max_val + padding])
        ax.set_aspect('equal')

        plt.tight_layout()
        plt.savefig(f'DMLP_train_test_prediction_performance_{safe_target}.png',
                    dpi=300,
                    bbox_inches='tight',
                    facecolor='white',
                    edgecolor='none')
        plt.close()

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8), dpi=300)  

    train_errors = metrics['y_train_pred'].ravel() - metrics['y_train_true'].ravel()
    test_errors = metrics['y_test_pred'].ravel() - metrics['y_test_true'].ravel()

    bp = ax1.boxplot([train_errors, test_errors],
                     labels=['Training', 'Test'],
                     patch_artist=True)

    bp['boxes'][0].set_facecolor('#0077BB')
    bp['boxes'][1].set_facecolor('#EE3377')

    ax1.set_ylabel('Prediction Error', fontsize=17, fontweight='bold')  # 12 -> 17
    ax1.set_title('DMLP Error Distribution Analysis\n' + target_column,
                  fontsize=19,  # 14 -> 19
                  fontweight='bold',
                  pad=20)
    ax1.tick_params(axis='both', labelsize=14)  
    ax1.grid(True, linestyle='--', alpha=0.3)
    ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)

    from scipy.stats import gaussian_kde

    def plot_density(data, color, label, ax):
        density = gaussian_kde(data)
        xs = np.linspace(min(data), max(data), 200)
        ax.plot(xs, density(xs), color=color, linewidth=2.5, label=label)
        ax.fill_between(xs, density(xs), alpha=0.2, color=color)

    plot_density(train_errors, '#0077BB', 'Training Error', ax2)
    plot_density(test_errors, '#EE3377', 'Test Error', ax2)

    ax2.set_xlabel('Prediction Error', fontsize=17, fontweight='bold')  # 12 -> 17
    ax2.set_ylabel('Density', fontsize=17, fontweight='bold')  # 12 -> 17
    ax2.set_title('DMLP Error Distribution Density\n' + target_column,
                  fontsize=19,  # 14 -> 19
                  fontweight='bold',
                  pad=20)
    ax2.tick_params(axis='both', labelsize=14)  
    ax2.grid(True, linestyle='--', alpha=0.3)
    ax2.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    ax2.legend(fontsize=14) 

    plt.tight_layout()
    plt.savefig(f'DMLP_error_analysis_{safe_target}.png',
                dpi=300,
                bbox_inches='tight',
                facecolor='white',
                edgecolor='none')
    plt.close()

    print(f"Graphics successfully recorded: {safe_target}")

def process_dataset_with_gridsearch(file_name, target_column):
    try:
        print(f"\nLoading dataset from {file_name}...")
        df = pd.read_excel(file_name)
        if df.empty:
            raise ValueError("Dataset is empty")

        feature_columns = [col for col in df.columns if col != target_column]
        if not feature_columns:
            raise ValueError("No feature columns found")

        print(f"Features: {feature_columns}")
        print(f"Target: {target_column}")

        X = df[feature_columns].values
        y = df[target_column].values

        if np.isnan(X).any() or np.isnan(y).any():
            raise ValueError("Dataset contains NaN values")

        scaler_X = MinMaxScaler()
        scaler_y = MinMaxScaler()

        X_normalized = scaler_X.fit_transform(X)
        y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()

        X_train, X_test, y_train, y_test = train_test_split(
            X_normalized, y_normalized, test_size=0.2, random_state=42, shuffle=True
        )

        param_grid = {
            'n_hidden_layers': [3, 5, 7, 10],
            'n_neurons': [8, 10, 12],
            'max_iter': [100, 150, 200],
            'patience': [10, 15, 20],
            'cv': [3, 5],
            'dropout_rate': [0.0001, 0.0005, 0.001, 0.002]
        }

        print("\nGrid Search Parameters:")
        for param, values in param_grid.items():
            print(f"{param}: {values}")

        best_score = float('-inf')
        best_cv = None
        best_params = None
        best_model = None

        for cv_value in param_grid['cv']:
            current_param_grid = param_grid.copy()
            del current_param_grid['cv']

            base_model = RPropMLPEstimator()
            grid_search = GridSearchCV(
                estimator=base_model,
                param_grid=current_param_grid,
                cv=cv_value,
                scoring='neg_mean_squared_error',
                n_jobs=1,
                verbose=1,
                refit=True
            )

            print(f"\nTrying CV={cv_value}...")
            grid_search.fit(X_normalized, y_normalized)

            if grid_search.best_score_ > best_score:
                best_score = grid_search.best_score_
                best_cv = cv_value
                best_params = grid_search.best_params_
                best_model = grid_search.best_estimator_

        print("\nOptimal parameters found:")
        print(f"Best CV value: {best_cv}")
        print("Best parameters:", best_params)
        print("Best cross-validation score:", -best_score)

        early_stopping_epoch = len(best_model.history['train_loss']) - best_params['patience']

        y_train_pred = best_model.predict(X_train)
        y_train_true = y_train.reshape(-1, 1)

        y_test_pred = best_model.predict(X_test)
        y_test_true = y_test.reshape(-1, 1)

        y_train_pred = scaler_y.inverse_transform(y_train_pred)
        y_train_true = scaler_y.inverse_transform(y_train_true)
        y_test_pred = scaler_y.inverse_transform(y_test_pred)
        y_test_true = scaler_y.inverse_transform(y_test_true)

        metrics = {
            'train_r2': r2_score(y_train_true, y_train_pred),
            'test_r2': r2_score(y_test_true, y_test_pred),
            'train_rmse': np.sqrt(mean_squared_error(y_train_true, y_train_pred)),
            'test_rmse': np.sqrt(mean_squared_error(y_test_true, y_test_pred)),
            'train_mae': mean_absolute_error(y_train_true, y_train_pred),
            'test_mae': mean_absolute_error(y_test_true, y_test_pred),
            'train_mape': np.mean(np.abs((y_train_true - y_train_pred) / y_train_true)) * 100,
            'test_mape': np.mean(np.abs((y_test_true - y_test_pred) / y_test_true)) * 100,
            'y_train_true': y_train_true,
            'y_train_pred': y_train_pred,
            'y_test_true': y_test_true,
            'y_test_pred': y_test_pred,
            'best_cv': best_cv  
        }
        train_results = pd.DataFrame({
            'Set': ['Train'] * len(y_train_true),
            'Actual_Values': y_train_true.ravel(),
            'Predicted_Values': y_train_pred.ravel()
        })

        test_results = pd.DataFrame({
            'Set': ['Test'] * len(y_test_true),
            'Actual_Values': y_test_true.ravel(),
            'Predicted_Values': y_test_pred.ravel()
        })

        all_results = pd.concat([train_results, test_results], ignore_index=True)

        safe_target = target_column.replace(" ", "_").replace("(", "").replace(")", "")
        results_filename = f'prediction_results_{safe_target}.xlsx'
        try:
            with pd.ExcelWriter(results_filename) as writer:
                train_results.to_excel(writer, sheet_name='Training Results', index=False)
                test_results.to_excel(writer, sheet_name='Test Results', index=False)
                all_results.to_excel(writer, sheet_name='All Results', index=False)

                cv_results = pd.DataFrame(grid_search.cv_results_)
                cv_results.to_excel(writer, sheet_name='CV Results', index=False)
        except Exception as e:
            print(f"Warning: Could not save to Excel: {str(e)}")

        create_publication_quality_plots(
            best_model=best_model,
            target_column=target_column,
            metrics=metrics,
            early_stopping_epoch=early_stopping_epoch
        )

        return metrics, best_params

    except Exception as e:
        print(f"Error processing dataset {target_column}: {str(e)}")
        raise

if __name__ == "__main__":
    try:
        datasets = [
            {
                'file': 'Dataset_WearLoss_10N.xlsx',
                'target': 'Wear Loss (10N)'
            },
            {
                'file': 'Dataset_WearLoss_20N.xlsx',
                'target': 'Wear Loss (20N)'
            },
            {
                'file': 'Dataset_WearLoss_30N.xlsx',
                'target': 'Wear Loss (30N)'
            }
        ]

        all_results = {}
        for dataset in datasets:
            print(f"\nProcessing {dataset['target']}...")
            metrics, best_params = process_dataset_with_gridsearch(
                dataset['file'],
                dataset['target']
            )

            all_results[dataset['target']] = {
                'metrics': metrics,
                'best_params': best_params
            }

            print(f"\nResults for {dataset['target']}:")
            print("Training Set Performance:")
            print(f"R² Score: {metrics['train_r2']:.4f}")
            print(f"RMSE: {metrics['train_rmse']:.8f}")
            print(f"MAE: {metrics['train_mae']:.8f}")
            print(f"MAPE: {metrics['train_mape']:.4f}%")

            print("\nTest Set Performance:")
            print(f"R² Score: {metrics['test_r2']:.4f}")
            print(f"RMSE: {metrics['test_rmse']:.8f}")
            print(f"MAE: {metrics['test_mae']:.8f}")
            print(f"MAPE: {metrics['test_mape']:.4f}%")

            print("\nBest Parameters:", best_params)

        metrics_df = pd.DataFrame()
        params_df = pd.DataFrame()

        for target, results in all_results.items():
            metrics_data = pd.DataFrame({
                'Metric': [
                    'Train R²', 'Test R²',
                    'Train RMSE', 'Test RMSE',
                    'Train MAE', 'Test MAE',
                    'Train MAPE', 'Test MAPE'
                ],
                target: [
                    results['metrics']['train_r2'],
                    results['metrics']['test_r2'],
                    results['metrics']['train_rmse'],
                    results['metrics']['test_rmse'],
                    results['metrics']['train_mae'],
                    results['metrics']['test_mae'],
                    results['metrics']['train_mape'],
                    results['metrics']['test_mape']
                ]
            }).set_index('Metric')

            metrics_df = pd.concat([metrics_df, metrics_df], axis=1)

            params_data = pd.DataFrame({
                'Parameter': list(results['best_params'].keys()),
                target: list(results['best_params'].values())
            }).set_index('Parameter')

            params_df = pd.concat([params_df, params_data], axis=1)

        with pd.ExcelWriter('final_results_summary.xlsx') as writer:
            metrics_df.to_excel(writer, sheet_name='All Metrics')
            params_df.to_excel(writer, sheet_name='Best Parameters')

        print("\nProcessing complete. Results have been saved to:")
        print("1. Individual prediction results: prediction_results_*.xlsx")
        print("2. Summary results: final_results_summary.xlsx")

    except Exception as e:
        print(f"Error in main execution: {str(e)}")


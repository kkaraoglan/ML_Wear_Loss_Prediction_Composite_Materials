import logging
import warnings
from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from abc import ABC, abstractmethod

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')


class PolynomialRegressionOptimizer:
    def __init__(self, min_degree=2, max_degree=5, n_splits=3, random_state=42):
        self.min_degree = min_degree
        self.max_degree = max_degree
        self.n_splits = n_splits
        self.random_state = random_state
        self.best_degree = None
        self.best_score = float('-inf')
        self.scores = {}
        self.detailed_metrics = {}

    def optimize(self, X: pd.DataFrame, y: pd.Series):
        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)

        for degree in range(self.min_degree, self.max_degree + 1):
            scores = []
            rmse_scores = []
            mae_scores = []

            poly = PolynomialFeatures(degree=degree, include_bias=False)

            for train_idx, val_idx in kf.split(X):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

                X_train_poly = poly.fit_transform(X_train)
                X_val_poly = poly.transform(X_val)

                model = LinearRegression()
                model.fit(X_train_poly, y_train)

                y_pred = model.predict(X_val_poly)
                scores.append(r2_score(y_val, y_pred))
                rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))
                mae_scores.append(mean_absolute_error(y_val, y_pred))

            avg_score = np.mean(scores)
            self.scores[degree] = avg_score

            self.detailed_metrics[degree] = {
                'R²': avg_score,
                'RMSE': np.mean(rmse_scores),
                'MAE': np.mean(mae_scores),
                'Std R²': np.std(scores)
            }

            if avg_score > self.best_score:
                self.best_score = avg_score
                self.best_degree = degree

        return self.best_degree, self.detailed_metrics

    def plot_degree_scores(self, save_dir: Optional[Path] = None):
        """
        Parameters:
        save_dir (Path, optional): Directory to save the plots
        """
        degrees = list(self.scores.keys())
        scores = list(self.scores.values())

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        ax1.plot(degrees, scores, 'bo-')
        ax1.axvline(x=self.best_degree, color='r', linestyle='--',
                    label=f'Best Degree = {self.best_degree}')
        ax1.set_xlabel('Polynomial Degree')
        ax1.set_ylabel('R² Score')
        ax1.set_title(f'Polynomial Regression (Degree {self.best_degree})\nR² Scores by Polynomial Degree')
        ax1.grid(True)
        ax1.legend()

        rmse_scores = [self.detailed_metrics[d]['RMSE'] for d in degrees]
        mae_scores = [self.detailed_metrics[d]['MAE'] for d in degrees]

        ax2.plot(degrees, rmse_scores, 'ro-', label='RMSE')
        ax2.plot(degrees, mae_scores, 'go-', label='MAE')
        ax2.set_xlabel('Polynomial Degree')
        ax2.set_ylabel('Error')
        ax2.set_title(f'Polynomial Regression (Degree {self.best_degree})\nError Metrics by Polynomial Degree')
        ax2.grid(True)
        ax2.legend()

        plt.tight_layout()

        if save_dir is not None:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)

            combined_plot_path = save_dir / f'polynomial_optimization_combined.png'
            plt.savefig(combined_plot_path, dpi=300, bbox_inches='tight')
            logger.info(f'Combined plot saved to {combined_plot_path}')

            plt.figure(figsize=(8, 6))

            plt.plot(degrees, scores, 'bo-')
            plt.axvline(x=self.best_degree, color='r', linestyle='--',
                        label=f'Best Degree = {self.best_degree}')
            plt.xlabel('Polynomial Degree')
            plt.ylabel('R² Score')
            plt.title(f'Polynomial Regression (Degree {self.best_degree})\nR² Scores by Polynomial Degree')
            plt.grid(True)
            plt.legend()
            r2_plot_path = save_dir / f'polynomial_optimization_r2.png'
            plt.savefig(r2_plot_path, dpi=300, bbox_inches='tight')
            logger.info(f'R² plot saved to {r2_plot_path}')
            plt.close()

            plt.figure(figsize=(8, 6))
            plt.plot(degrees, rmse_scores, 'ro-', label='RMSE')
            plt.plot(degrees, mae_scores, 'go-', label='MAE')
            plt.xlabel('Polynomial Degree')
            plt.ylabel('Error')
            plt.title(f'Polynomial Regression (Degree {self.best_degree})\nError Metrics by Polynomial Degree')
            plt.grid(True)
            plt.legend()
            error_plot_path = save_dir / f'polynomial_optimization_errors.png'
            plt.savefig(error_plot_path, dpi=300, bbox_inches='tight')
            logger.info(f'Error metrics plot saved to {error_plot_path}')
            plt.close()

        plt.show()
        plt.close()

        print(f"\nBest Polynomial Degree: {self.best_degree}")
        print("\nDetailed metrics for each degree:")
        for degree in degrees:
            metrics = self.detailed_metrics[degree]
            print(f"\nDegree {degree}:")
            print(f"R² = {metrics['R²']:.4f} (±{metrics['Std R²']:.4f})")
            print(f"RMSE = {metrics['RMSE']:.4f}")
            print(f"MAE = {metrics['MAE']:.4f}")


def plot_prediction_results(self, model_name: str, X: pd.DataFrame, y: pd.Series,
                            save_dir: Optional[Path] = None) -> None:
    try:
        all_predictions = np.array([])
        all_actual = np.array([])

        kf = KFold(n_splits=self.config.n_splits, shuffle=True, random_state=self.config.random_state)
        fold_metrics = []

        train_actual_all = np.array([])
        train_predictions_all = np.array([])

        test_actual_all = np.array([])
        test_predictions_all = np.array([])

        for train_idx, test_idx in kf.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            model, is_poly = self.models[model_name]
            if isinstance(model, LinearRegression):
                current_model = LinearRegression()
            elif isinstance(model, GradientBoostingRegressor):
                current_model = GradientBoostingRegressor(**self.config.gb_params)
            elif isinstance(model, RandomForestRegressor):
                current_model = RandomForestRegressor(**self.config.rf_params)

            if is_poly:
                X_train_processed = self.model_trainer.poly.fit_transform(X_train)
                X_test_processed = self.model_trainer.poly.transform(X_test)
            else:
                X_train_processed = X_train
                X_test_processed = X_test

            current_model.fit(X_train_processed, y_train)

            y_test_pred = current_model.predict(X_test_processed)
            test_predictions_all = np.append(test_predictions_all, y_test_pred)
            test_actual_all = np.append(test_actual_all, y_test)

            y_train_pred = current_model.predict(X_train_processed)
            train_predictions_all = np.append(train_predictions_all, y_train_pred)
            train_actual_all = np.append(train_actual_all, y_train)

            metrics = MetricsCalculator.calculate_metrics(y_test, y_test_pred)
            fold_metrics.append(metrics)

        avg_metrics = {
            metric: np.mean([m[metric] for m in fold_metrics])
            for metric in fold_metrics[0].keys()
        }

        plt.figure(figsize=(12, 10))  

        plt.rcParams.update({'font.size': 14})  

        plt.scatter(train_actual_all[:16], train_predictions_all[:16],
                    c='green', alpha=0.7,
                    label='Train Data', marker='o', s=120)

        plt.scatter(test_actual_all[:4], test_predictions_all[:4],
                    c='blue', alpha=0.7,
                    label='Test Data', marker='x', s=120)

        min_val = min(train_actual_all.min(), train_predictions_all.min(),
                      test_actual_all.min(), test_predictions_all.min())
        max_val = max(train_actual_all.max(), train_predictions_all.max(),
                      test_actual_all.max(), test_predictions_all.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'k--',
                 label='Perfect Prediction', linewidth=2)

        plt.xlabel('Experimental Values', fontsize=17, fontweight='bold')
        plt.ylabel('Predicted Values', fontsize=17, fontweight='bold')
        plt.title(f'{model_name} - Experimental vs Predicted Values\n'
                  f'(Scaling: {self.config.scaling_method})',
                  fontsize=19, fontweight='bold', pad=20)

        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)

        metrics_text = (
            f'R² = {avg_metrics["R²"]:.4f}\n'
            f'RMSE = {avg_metrics["RMSE"]:.4f}\n'
            f'MAE = {avg_metrics["MAE"]:.4f}\n'
            f'MAPE = {avg_metrics["MAPE"]:.2f}%'
        )
        plt.text(0.95, 0.05, metrics_text,
                 transform=plt.gca().transAxes,
                 verticalalignment='bottom',
                 horizontalalignment='right',
                 fontsize=17, 
                 fontweight='bold',
                 fontname='Arial',
                 bbox=dict(boxstyle='round',
                           facecolor='white',
                           alpha=0.8,
                           pad=0.8))  

        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(fontsize=14, loc='upper left', frameon=True,
                   facecolor='white', edgecolor='gray',
                   shadow=True)
        plt.tight_layout()

        if save_dir is not None:
            save_path = save_dir / f'{model_name.lower().replace(" ", "_")}_prediction_plot.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f'Plot saved to {save_path}')

        plt.show()
        plt.close()

    except Exception as e:
        logger.error(f"Error in plotting predictions: {str(e)}")
        raise

def plot_all_models(self, X: pd.DataFrame, y: pd.Series, save_dir: Optional[Path] = None) -> None:
    for model_name in self.models.keys():
        self.plot_prediction_results(model_name, X, y, save_dir=save_dir)

class ScalingMethod(Enum):
    NONE = "none"
    STANDARD = "standard"
    MINMAX = "minmax"

    def __str__(self):
        return self.value


@dataclass(frozen=True)
class ModelConfig:
    n_splits: int
    random_state: int
    polynomial_degree: int
    scaling_method: ScalingMethod
    validation_size: float
    early_stopping_rounds: int
    gb_params: Dict[str, Any]
    rf_params: Dict[str, Any]

    def __post_init__(self):
        if not isinstance(self.n_splits, int) or self.n_splits < 2:
            raise ValueError("n_splits must be an integer >= 2")
        if not isinstance(self.random_state, int):
            raise ValueError("random_state must be an integer")
        if not isinstance(self.polynomial_degree, int) or self.polynomial_degree < 1:
            raise ValueError("polynomial_degree must be an integer >= 1")
        if not isinstance(self.validation_size, float) or not 0 < self.validation_size < 1:
            raise ValueError("validation_size must be a float between 0 and 1")
        if not isinstance(self.early_stopping_rounds, int) or self.early_stopping_rounds < 1:
            raise ValueError("early_stopping_rounds must be a positive integer")


class MetricsCalculator:
    @staticmethod
    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        if len(y_true) != len(y_pred):
            raise ValueError("Length mismatch between y_true and y_pred")
        if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)):
            raise ValueError("Input contains NaN values")

        mape = np.mean(np.abs((y_true - y_pred) / np.maximum(1e-10, np.abs(y_true)))) * 100

        return {
            'R²': float(r2_score(y_true, y_pred)),
            'RMSE': float(np.sqrt(mean_squared_error(y_true, y_pred))),
            'MAE': float(mean_absolute_error(y_true, y_pred)),
            'MAPE': float(mape)
        }


class DataProcessor:
    def __init__(self, scaling_method: ScalingMethod):
        self.scaling_method = scaling_method
        self.scaler = self._get_scaler()
        self.feature_names: List[str] = []
        self._is_fitted = False

    def _get_scaler(self) -> Optional[BaseEstimator]:
        if self.scaling_method == ScalingMethod.STANDARD:
            return StandardScaler()
        elif self.scaling_method == ScalingMethod.MINMAX:
            return MinMaxScaler()
        return None

    def load_and_preprocess(self, file_path: Path) -> Tuple[pd.DataFrame, pd.Series]:
        try:
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")

            logger.info(f"Loading data from {file_path}")
            df = pd.read_excel(file_path)

            self._validate_data(df)

            X = df.iloc[:, :4]  
            y = df.iloc[:, 4]  

            self.feature_names = X.columns.tolist()

            X_processed = self._scale_features(X)

            logger.info("Data preprocessing completed successfully")
            return X_processed, y

        except Exception as e:
            logger.error(f"Error in data processing: {str(e)}")
            raise

    def _validate_data(self, df: pd.DataFrame) -> None:
        if df.empty:
            raise ValueError("Empty dataset")

        if df.shape[1] < 5:
            raise ValueError(f"Dataset must have at least 5 columns, found {df.shape[1]}")

        if df.isnull().any().any():
            null_counts = df.isnull().sum()
            raise ValueError(f"Dataset contains missing values:\n{null_counts}")

        if not all(df.dtypes.apply(lambda x: np.issubdtype(x, np.number))):
            non_numeric = df.dtypes[~df.dtypes.apply(lambda x: np.issubdtype(x, np.number))]
            raise ValueError(f"All columns must be numeric. Non-numeric columns:\n{non_numeric}")

        if df.std().min() == 0:
            zero_var_cols = df.columns[df.std() == 0].tolist()
            raise ValueError(f"Zero variance detected in columns: {zero_var_cols}")

    def _scale_features(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.scaler is None:
            return X.copy()

        try:
            if not self._is_fitted:
                X_scaled = pd.DataFrame(
                    self.scaler.fit_transform(X),
                    columns=X.columns,
                    index=X.index
                )
                self._is_fitted = True
            else:
                X_scaled = pd.DataFrame(
                    self.scaler.transform(X),
                    columns=X.columns,
                    index=X.index
                )

            if X_scaled.isnull().any().any():
                raise ValueError("Scaling produced NaN values")

            return X_scaled

        except Exception as e:
            logger.error(f"Error during feature scaling: {str(e)}")
            raise


class ModelTrainer:
    def __init__(self, config: ModelConfig):
        self.config = config
        self.poly = PolynomialFeatures(
            degree=config.polynomial_degree,
            include_bias=False
        )

    def train_and_evaluate(
            self,
            model: BaseEstimator,
            X_train: pd.DataFrame,
            X_test: pd.DataFrame,
            y_train: pd.Series,
            y_test: pd.Series,
            is_polynomial: bool = False
    ) -> Dict[str, Dict[str, float]]:
        try:
            if is_polynomial:
                X_train_processed = self.poly.fit_transform(X_train)
                X_test_processed = self.poly.transform(X_test)
            else:
                X_train_processed = X_train
                X_test_processed = X_test

            self._validate_transformed_data(X_train_processed, X_test_processed)

            model.fit(X_train_processed, y_train)

            y_train_pred = model.predict(X_train_processed)
            y_test_pred = model.predict(X_test_processed)

            train_metrics = MetricsCalculator.calculate_metrics(y_train, y_train_pred)
            test_metrics = MetricsCalculator.calculate_metrics(y_test, y_test_pred)

            return {
                'train': train_metrics,
                'test': test_metrics
            }

        except Exception as e:
            logger.error(f"Error in model training/evaluation: {str(e)}")
            raise

    def _validate_transformed_data(self, X_train: np.ndarray, X_test: np.ndarray) -> None:
        if np.any(np.isnan(X_train)) or np.any(np.isnan(X_test)):
            raise ValueError("NaN values detected in transformed features")
        if np.any(np.isinf(X_train)) or np.any(np.isinf(X_test)):
            raise ValueError("Infinite values detected in transformed features")


class WearLossPredictor:
    def __init__(self, config: ModelConfig):
        self.config = config
        self.data_processor = DataProcessor(config.scaling_method)
        self.model_trainer = ModelTrainer(config)
        self.models = self._initialize_models()
        self.results: Dict[str, Dict] = {}

    def _initialize_models(self) -> Dict[str, Tuple[BaseEstimator, bool]]:
        return {
            'Linear Regression': (LinearRegression(), False),
            'Polynomial Regression': (LinearRegression(), True),
            'Gradient Boosting': (GradientBoostingRegressor(**self.config.gb_params), False),
            'Random Forest': (RandomForestRegressor(**self.config.rf_params), False)
        }

    def plot_prediction_results(self, model_name: str, X: pd.DataFrame, y: pd.Series,
                                save_dir: Optional[Path] = None) -> None:
        try:
            all_predictions = np.array([])
            all_actual = np.array([])

            kf = KFold(n_splits=self.config.n_splits, shuffle=True, random_state=self.config.random_state)
            fold_metrics = []

            train_actual_all = np.array([])
            train_predictions_all = np.array([])

            test_actual_all = np.array([])
            test_predictions_all = np.array([])

            for train_idx, test_idx in kf.split(X):
                X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
                y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

                model, is_poly = self.models[model_name]
                if isinstance(model, LinearRegression):
                    current_model = LinearRegression()
                elif isinstance(model, GradientBoostingRegressor):
                    current_model = GradientBoostingRegressor(**self.config.gb_params)
                elif isinstance(model, RandomForestRegressor):
                    current_model = RandomForestRegressor(**self.config.rf_params)

                if is_poly:
                    X_train_processed = self.model_trainer.poly.fit_transform(X_train)
                    X_test_processed = self.model_trainer.poly.transform(X_test)
                else:
                    X_train_processed = X_train
                    X_test_processed = X_test

                current_model.fit(X_train_processed, y_train)

                y_test_pred = current_model.predict(X_test_processed)
                test_predictions_all = np.append(test_predictions_all, y_test_pred)
                test_actual_all = np.append(test_actual_all, y_test)

                y_train_pred = current_model.predict(X_train_processed)
                train_predictions_all = np.append(train_predictions_all, y_train_pred)
                train_actual_all = np.append(train_actual_all, y_train)

                metrics = MetricsCalculator.calculate_metrics(y_test, y_test_pred)
                fold_metrics.append(metrics)

            avg_metrics = {
                metric: np.mean([m[metric] for m in fold_metrics])
                for metric in fold_metrics[0].keys()
            }

            plt.figure(figsize=(10, 8))

            plt.scatter(train_actual_all[:16], train_predictions_all[:16], c='green', alpha=0.7,
                        label='Train Data', marker='o', s=100)  

            plt.scatter(test_actual_all[:4], test_predictions_all[:4], c='blue', alpha=0.7,
                        label='Test Data', marker='x', s=100) 

            min_val = min(train_actual_all.min(), train_predictions_all.min(),
                          test_actual_all.min(), test_predictions_all.min())
            max_val = max(train_actual_all.max(), train_predictions_all.max(),
                          test_actual_all.max(), test_predictions_all.max())
            plt.plot([min_val, max_val], [min_val, max_val], 'k--', label='Perfect Prediction')

            plt.xlabel('Experimental Values', fontsize=14, fontweight='bold')
            plt.ylabel('Predicted Values', fontsize=14, fontweight='bold')
            plt.title(f'{model_name} - Experimental vs Predicted Values\n'
                      f'(Scaling: {self.config.scaling_method})', fontsize=16, fontweight='bold')

            metrics_text = (
                f'R² = {avg_metrics["R²"]:.4f}\n'
                f'RMSE = {avg_metrics["RMSE"]:.4f}\n'
                f'MAE = {avg_metrics["MAE"]:.4f}\n'
                f'MAPE = {avg_metrics["MAPE"]:.2f}%'
            )
            plt.text(0.95, 0.05, metrics_text,
                     transform=plt.gca().transAxes,
                     verticalalignment='bottom',
                     horizontalalignment='right',
                     fontsize=14,
                     fontweight='bold',  
                     fontname='Arial',  
                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            plt.grid(True, linestyle='--', alpha=0.7)
            plt.legend(fontsize=12, loc='upper left', frameon=False)
            plt.tight_layout()

            if save_dir is not None:
                save_path = save_dir / f'{model_name.lower().replace(" ", "_")}_prediction_plot.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f'Plot saved to {save_path}')

            plt.show()
            plt.close()

        except Exception as e:
            logger.error(f"Error in plotting predictions: {str(e)}")
            raise

    def plot_all_models(self, X: pd.DataFrame, y: pd.Series, save_dir: Optional[Path] = None) -> None:
        for model_name in self.models.keys():
            self.plot_prediction_results(model_name, X, y, save_dir=save_dir)

    def run_analysis(self, file_path: Path) -> pd.DataFrame:
        try:
            X, y = self.data_processor.load_and_preprocess(file_path)
            save_dir = Path("results")
            file_name = file_path.stem 
            plots_dir = save_dir / f"{file_name}_plots"
            plots_dir.mkdir(parents=True, exist_ok=True)

            if any(model_name == 'Polynomial Regression' for model_name in self.models.keys()):
                logger.info("Optimizing polynomial regression degree...")
                optimizer = PolynomialRegressionOptimizer(
                    min_degree=2,
                    max_degree=5,
                    n_splits=self.config.n_splits,
                    random_state=self.config.random_state
                )
                best_degree, detailed_metrics = optimizer.optimize(X, y)

                poly_plots_dir = plots_dir / 'polynomial_optimization'
                optimizer.plot_degree_scores(save_dir=poly_plots_dir)

                self.config = ModelConfig(
                    n_splits=self.config.n_splits,
                    random_state=self.config.random_state,
                    polynomial_degree=best_degree,  
                    scaling_method=self.config.scaling_method,
                    validation_size=self.config.validation_size,
                    early_stopping_rounds=self.config.early_stopping_rounds,
                    gb_params=self.config.gb_params,
                    rf_params=self.config.rf_params
                )

                self.model_trainer = ModelTrainer(self.config)

            kf = KFold(
                n_splits=self.config.n_splits,
                shuffle=True,
                random_state=self.config.random_state
            )

            model_results = []

            for model_name, (model, is_polynomial) in self.models.items():
                logger.info(f"Evaluating {model_name}")
                fold_results = self._evaluate_model(model, is_polynomial, X, y, kf)
                model_results.append({
                    'Model': model_name,
                    **self._format_results(fold_results)
                })

            results_df = pd.DataFrame(model_results)
            self.results[str(self.config.scaling_method)] = results_df

            return results_df

        except Exception as e:
            logger.error(f"Error in analysis pipeline: {str(e)}")
            raise

    def _evaluate_model(
            self,
            model: BaseEstimator,
            is_polynomial: bool,
            X: pd.DataFrame,
            y: pd.Series,
            kf: KFold
    ) -> List[Dict[str, Dict[str, float]]]:
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(kf.split(X), 1):
            logger.info(f"Processing fold {fold}/{self.config.n_splits}")

            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            if isinstance(model, LinearRegression):
                model_instance = LinearRegression()
            elif isinstance(model, GradientBoostingRegressor):
                model_instance = GradientBoostingRegressor(**self.config.gb_params)
            elif isinstance(model, RandomForestRegressor):
                model_instance = RandomForestRegressor(**self.config.rf_params)

            fold_metrics = self.model_trainer.train_and_evaluate(
                model_instance, X_train, X_test, y_train, y_test, is_polynomial
            )
            fold_results.append(fold_metrics)

        return fold_results

    def _format_results(self, fold_results: List[Dict[str, Dict[str, float]]]) -> Dict[str, str]:
        formatted_results = {}

        for split in ['train', 'test']:
            for metric in ['R²', 'RMSE', 'MAE', 'MAPE']:
                values = [fold[split][metric] for fold in fold_results]
                formatted_results[f"{split.capitalize()} {metric} (mean ± std)"] = (
                    f"{np.mean(values):.4f} ± {np.std(values):.4f}"
                )

        return formatted_results


def main():
    try:
        fileName = "Dataset_WearLoss_30N"
        file_path = Path(fileName + ".xlsx")
        output_dir = Path("results")
        output_dir.mkdir(parents=True, exist_ok=True)

        plots_dir = output_dir / f"{fileName}_plots"
        plots_dir.mkdir(exist_ok=True)

        file_handler = logging.FileHandler(output_dir / f"{fileName}_analysis.log")
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        ))
        logger.addHandler(file_handler)

        logger.info("Starting wear loss analysis")

        config = ModelConfig(
            n_splits=3,
            random_state=42,
            polynomial_degree=2,
            scaling_method=ScalingMethod.STANDARD,
            validation_size=0.2,
            early_stopping_rounds=20,
            gb_params={
                'n_estimators': 1000,
                'learning_rate': 0.05,
                'max_depth': 4,  
                'subsample': 0.8,
                'random_state': 42
            },
            rf_params={
                'n_estimators': 1000,
                'max_depth': None, # Depth limit can be added according to your results. Important for avoid overfitting 'max_depth': 4,
                'oob_score': True,
                'warm_start': True,
                'random_state': 42
 # The following parameters can be used to prevent overfitting.                
               # 'min_samples_split': 5,
               # 'min_samples_leaf': 3,
               # 'max_features': 'sqrt',  # Limit the number of features
            }
        )

        predictor = WearLossPredictor(config)
        logger.info("Running analysis with standard scaling")
        results = predictor.run_analysis(file_path)

        X, y = predictor.data_processor.load_and_preprocess(file_path)

        logger.info("Generating prediction plots")
        predictor.plot_all_models(X, y, save_dir=plots_dir)

        results_file = Path(output_dir) / f"{fileName}_model_results.csv"
        results.to_csv(results_file, index=False)
        logger.info(f"Results saved to {results_file}")

        excel_file = output_dir / f"{fileName}_model_results.xlsx"
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            results.to_excel(writer, index=False, sheet_name='Results')
            workbook = writer.book
            worksheet = writer.sheets['Results']

            for column in worksheet.columns:
                max_length = 0
                column = list(column)
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = (max_length + 2)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width

        logger.info(f"Formatted results saved to {excel_file}")

        print("\nModel Evaluation Results:")
        print("=" * 100)
        print(results.to_string(index=False))
        print("\nAnalysis completed successfully!")

        if input("\nWould you like to compare different scaling methods? (y/n): ").lower() == 'y':
            scaling_results = {}

            for scaling_method in ScalingMethod:
                logger.info(f"Running analysis with {scaling_method.value} scaling")
                config = ModelConfig(
                    n_splits=3,
                    random_state=42,
                    polynomial_degree=2,
                    scaling_method=scaling_method,
                    validation_size=0.2,
                    early_stopping_rounds=20,
                    gb_params=config.gb_params,
                    rf_params=config.rf_params
                )

                predictor = WearLossPredictor(config)
                scaling_results[scaling_method.value] = predictor.run_analysis(file_path)

                X, y = predictor.data_processor.load_and_preprocess(file_path)
                scaling_plots_dir = plots_dir / f"scaling_{scaling_method.value}"
                scaling_plots_dir.mkdir(exist_ok=True)
                predictor.plot_all_models(X, y, save_dir=scaling_plots_dir)

            comparative_dir = output_dir / f"{fileName}_scaling_comparison"
            comparative_dir.mkdir(exist_ok=True)

            for scaling_method, results_df in scaling_results.items():
                results_df.to_csv(
                    comparative_dir / f"results_{scaling_method}.csv",
                    index=False
                )

            logger.info("Scaling comparison completed")
            print("\nScaling comparison results and plots have been saved to the respective directories")

    except Exception as e:
        logger.error(f"Error in main execution: {str(e)}")
        print(f"\nError occurred: {str(e)}")
        raise
    finally:
        logger.removeHandler(file_handler)
        file_handler.close()


if __name__ == "__main__":
    main()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping
from keras.initializers import RandomUniform
import numpy as np

# File paths and titles
file_paths = ['Book1.xlsx', 'Book2.xlsx', 'Book3.xlsx']
titles = {
    'Book1.xlsx': 'Wear Loss Prediction at 30N Load during Training',
    'Book2.xlsx': 'Wear Loss Prediction at 20N Load during Training',
    'Book3.xlsx': 'Wear Loss Prediction at 10N Load during Training'
}

def plot_loss_curve(history, file_path, early_stopping_epoch, best_val_loss):
    """Plots the loss function curves for both training and validation losses"""
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], color='#1f77b4', linewidth=3, marker='o', markersize=7, label='Training Loss')
    plt.plot(history.history['val_loss'], color='#ff7f0e', linewidth=3, marker='o', markersize=7, label='Validation Loss')

    # Highlight EarlyStopping best epoch
    plt.axvline(x=early_stopping_epoch, color='green', linestyle='--', label=f'Early Stopping Epoch ({early_stopping_epoch})')

    plt.title(titles[file_path], fontsize=18, fontweight='bold', color='#333333', family='Arial')
    plt.xlabel('Epochs', fontsize=14, fontweight='bold', color='#333333', family='Arial')
    plt.ylabel('Loss', fontsize=14, fontweight='bold', color='#333333', family='Arial')
    plt.grid(True, linestyle=':', color='gray', alpha=0.3)
    plt.legend()
    plt.savefig(f'loss_function_training_{file_path.split(".")[0]}.png', dpi=600, bbox_inches='tight')
    plt.show()

def train_mlp_model(X_train, y_train, X_test, y_test):
    """Defines and trains the MLP model with dropout for regularization"""
    model = Sequential()

    # Hidden layers and Dropout
    initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=42)
    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_initializer=initializer))  # First layer
    model.add(Dropout(0.2))  # Dropout rate set to 20%

    # Adding 4 more hidden layers (each with 10 neurons)
    for _ in range(4):
        model.add(Dense(10, activation='relu', kernel_initializer=initializer))
        model.add(Dropout(0.2))  # Dropout rate set to 20%

    # Output layer
    model.add(Dense(1))

    # Compile the model
    model.compile(optimizer='adam', loss='mean_squared_error')

    # Early stopping callback
    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

    # Train the model
    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),
                        callbacks=[early_stopping], verbose=0)

    # Get the epoch where early stopping occurred and the best validation loss
    early_stopping_epoch = np.argmin(history.history['val_loss'])
    best_val_loss = np.min(history.history['val_loss'])

    # Training predictions and loss
    train_predictions = model.predict(X_train)
    train_loss = mean_squared_error(y_train, train_predictions)

    return model, history, train_loss, early_stopping_epoch, best_val_loss

def process_file(file_path):
    """Processes each file by separating features and target, then trains the model"""
    try:
        # Read the Excel file
        data = pd.read_excel(file_path)
        # Shuffle the data
        data = data.sample(frac=1, random_state=42).reset_index(drop=True)
        print(f"Data for {file_path}:")
        print(data.head())

        # Separate features and target variable
        X = data.iloc[:, :4]
        y = data.iloc[:, -1]

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Scale the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Train the model and get the loss and early stopping details
        model, history, train_loss, early_stopping_epoch, best_val_loss = train_mlp_model(X_train, y_train, X_test, y_test)

        # Plot the loss function curve with early stopping details
        plot_loss_curve(history, file_path, early_stopping_epoch, best_val_loss)

        # Output results
        print(f"Model's training loss (Training Loss): {train_loss}")
        print(f"Best validation loss: {best_val_loss} at epoch {early_stopping_epoch}")

    except Exception as e:
        print(f"Error occurred while processing {file_path}: {e}")

# Run the process for each file
for file_path in file_paths:
    process_file(file_path)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import RandomUniform
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
import numpy as np

# File paths and titles
file_paths = ['Book1.xlsx', 'Book2.xlsx', 'Book3.xlsx']
titles = {
    'Book1.xlsx': 'Wear Loss Prediction at 30N Load during Training',
    'Book2.xlsx': 'Wear Loss Prediction at 20N Load during Training',
    'Book3.xlsx': 'Wear Loss Prediction at 10N Load during Training'
}

def plot_loss_curve(history, file_path, early_stopping_epoch, best_val_loss):
    """Plots the loss function curves for both training and validation losses"""
    plt.figure(figsize=(10, 6))
    plt.plot(history.history['loss'], color='#1f77b4', linewidth=3, marker='o', markersize=7, label='Training Loss')
    plt.plot(history.history['val_loss'], color='#ff7f0e', linewidth=3, marker='o', markersize=7, label='Validation Loss')

    # Highlight EarlyStopping best epoch
    plt.axvline(x=early_stopping_epoch, color='green', linestyle='--', label=f'Early Stopping Epoch ({early_stopping_epoch})')

    plt.title(titles[file_path], fontsize=18, fontweight='bold', color='#333333', family='Arial')
    plt.xlabel('Epochs', fontsize=14, fontweight='bold', color='#333333', family='Arial')
    plt.ylabel('Loss', fontsize=14, fontweight='bold', color='#333333', family='Arial')
    plt.grid(True, linestyle=':', color='gray', alpha=0.3)
    plt.legend()
    plt.savefig(f'loss_function_training_{file_path.split(".")[0]}.png', dpi=600, bbox_inches='tight')
    plt.show()

def create_mlp_model(dropout_rate=0.2):
    """Defines the MLP model with the specified dropout rate for regularization"""
    model = Sequential()
    initializer = RandomUniform(minval=-0.05, maxval=0.05, seed=42)
    model.add(Dense(10, input_dim=4, activation='relu', kernel_initializer=initializer))
    model.add(Dropout(dropout_rate))

    for _ in range(4):  # Adding additional hidden layers
        model.add(Dense(10, activation='relu', kernel_initializer=initializer))
        model.add(Dropout(dropout_rate))

    model.add(Dense(1))  # Output layer
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def perform_grid_search(X_train, y_train):
    """Performs GridSearchCV on the DMLP model"""
    model = KerasRegressor(build_fn=create_mlp_model, epochs=100, batch_size=32, verbose=0)

    param_grid = {
        'dropout_rate': [0.2, 0.3],
        'epochs': [100, 150],
        'batch_size': [32, 64]
    }

    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')
    grid_result = grid.fit(X_train, y_train)
    print("Best parameters found: ", grid_result.best_params_)
    print("Best score (MSE): ", -grid_result.best_score_)
    return grid_result.best_params_

def process_file(file_path):
    """Processes each file by separating features and target, then trains the model"""
    try:
        # Read the Excel file
        data = pd.read_excel(file_path)
        # Shuffle the data
        data = data.sample(frac=1, random_state=42).reset_index(drop=True)
        print(f"Data for {file_path}:")
        print(data.head())

        # Separate features and target variable
        X = data.iloc[:, :4]
        y = data.iloc[:, -1]

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Scale the features
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        # Perform Grid Search to get best hyperparameters
        best_params = perform_grid_search(X_train, y_train)

        # Train final model with best parameters
        model = create_mlp_model(dropout_rate=best_params['dropout_rate'])
        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

        history = model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], 
                            validation_data=(X_test, y_test), callbacks=[early_stopping], verbose=0)

        # Get best validation loss
        early_stopping_epoch = np.argmin(history.history['val_loss'])
        best_val_loss = np.min(history.history['val_loss'])

        # Plot the loss function curve
        plot_loss_curve(history, file_path, early_stopping_epoch, best_val_loss)

    except Exception as e:
        print(f"Error occurred while processing {file_path}: {e}")

# Run the process for each file
for file_path in file_paths:
    process_file(file_path)
